# ==============================================================================
# EXAMPLE ENVIRONMENT VARIABLES
# ==============================================================================
# Copy this file to .env and fill in your actual credentials.
# ==============================================================================

# --- Telegram API Credentials ---
# Get these from my.telegram.org for user account authentication.
API_ID=1234567
API_HASH=a1b2c3d4e5f6a1b2c3d4e5f6a1b2c3d4

# --- Telegram Bot Configuration ---
# The token for your Telegram Bot from BotFather.
TELEGRAM_BOT_TOKEN=1234567890:ABC-DEF1234ghIkl-zyx57W2v1u123ew11
# Comma-separated list of group/channel IDs the bot should monitor.
GROUP_IDS=-1001234567890,-1009876543210

# --- Telegram Session Configuration ---
# The name of the .session file that Telethon will create to store session data.
SESSION_NAME=ragbot_session
# Your phone number and password (if 2FA is enabled) for the Telegram user account.
TELEGRAM_PHONE=+15551234567
TELEGRAM_PASSWORD=your_2fa_password_here

# --- Redis Cache Configuration ---
# Hostname for the Redis server used for caching.
REDIS_HOST=localhost
# Port for the Redis server. (Note: LiteLLM proxy examples sometimes use 6380)
REDIS_PORT=6380
# Password for the Redis server, if required.
REDIS_PASSWORD=your_redis_password

# --- LiteLLM Proxy Configuration ---
# The URL of your LiteLLM proxy, which manages LLM API calls.
LITELLM_PROXY_URL=http://0.0.0.0:4000
# The API key for authenticating with the LiteLLM proxy itself, if needed.
LITELLM_PROXY_API_KEY=your_litellm_proxy_key

# --- Gemini API Key ---
# Your Google Gemini API key. This is often managed by the LiteLLM proxy.
GEMINI_API_KEY=your_gemini_api_key_here

# --- Knowledge Synthesis Script Configuration ---
# Maximum number of concurrent threads for processing data batches.
MAX_WORKERS=5
# The rate limit for API requests to the LLM (requests per minute).
REQUESTS_PER_MINUTE=90
# NOTE: Run src/scripts/check_litellm_setup.py to get a proxy-informed
# recommendation for REQUESTS_PER_MINUTE based on keys configured in
# utils/litellm_config.yaml. Use a safety margin (e.g. 80%).
# The number of conversations to process in a single batch.
BATCH_SIZE=2

# --- Local Caching (development only) ---
# Set to 'false' in production to rely on the LiteLLM proxy + Redis cache.
USE_LOCAL_FILE_CACHE=false

# --- RAG Pipeline Configuration ---
# Weights for re-ranking search results. These should sum to 1.0.
# The weight given to the semantic similarity score.
SEMANTIC_SCORE_WEIGHT=0.5
# The weight given to the recency of the information.
RECENCY_SCORE_WEIGHT=0.3
# The weight given to the status of the knowledge nugget (e.g., FACT, OPINION).
STATUS_SCORE_WEIGHT=0.2

# --- Conversation Grouping ---
# The maximum time in seconds between messages to be considered part of the same conversation.
CONVERSATION_TIME_THRESHOLD_SECONDS=300
# The maximum duration of a single conversation session in seconds.
SESSION_WINDOW_SECONDS=3600
